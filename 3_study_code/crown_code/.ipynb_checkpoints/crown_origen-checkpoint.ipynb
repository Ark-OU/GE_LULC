{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, zipfile, io, re\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import sqrt\n",
    "import optuna\n",
    "from optuna import integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keras, TensorFlow ---------------\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D, BatchNormalization, Convolution2D, Input\n",
    "from keras import optimizers\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings('ignore')\n",
    "gpus = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO Functions ------------------------------\n",
    "def pkl_saver(object, pkl_filename):\n",
    "    with open(pkl_filename, 'wb') as web:\n",
    "        pickle.dump(object , web)\n",
    "\n",
    "\n",
    "def pkl_loader(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as web:\n",
    "        data = pickle.load(web)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Dir generator ----------------------------\n",
    "def dir_generator():\n",
    "    timename = '{0:%Y%m%d%H%M}'.format(datetime.datetime.now())\n",
    "    os.mkdir(os.path.join(os.getcwd(), timename))\n",
    "    os.chdir(os.path.join(os.getcwd(), timename))\n",
    "    result_img_dir = './result_img/'\n",
    "    if os.path.exists(result_img_dir) == False:\n",
    "        os.mkdir(result_img_dir)\n",
    "    model_dir = './model/'\n",
    "    if os.path.exists(model_dir) == False:\n",
    "        os.mkdir(model_dir)\n",
    "    weights_dir = './weights/'\n",
    "    if os.path.exists(weights_dir) == False:\n",
    "        os.mkdir(weights_dir)\n",
    "    logging_dir = './logs/'\n",
    "    if os.path.exists(logging_dir) == False:\n",
    "        os.mkdir(logging_dir)\n",
    "\n",
    "\n",
    "# Data Loader ----------------------------------\n",
    "def crown_DataLoader(zip_name):\n",
    "    z = zipfile.ZipFile(zip_name)\n",
    "    imgfiles = [x for x in z.namelist()]\n",
    "    #imgfiles = [x for x in z.namelist() if re.search(r'^' + zip_name.split('.')[0] + '.tif$', x)]\n",
    "    filenames = []\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    point = []\n",
    "    max_light = 0\n",
    "    print('NTL_processing...')\n",
    "    ext = ('.tif')\n",
    "    for imgfile in tqdm(imgfiles):\n",
    "        if imgfile.endswith(ext):\n",
    "            print(imgfile)\n",
    "            image = Image.open(io.BytesIO(z.read(imgfile)))\n",
    "            data = np.asarray(image).reshape(image_size,image_size,-1)\n",
    "            file = os.path.basename(imgfile)\n",
    "            file_split = [i for i in file.split('_')]\n",
    "            y = float(os.path.splitext(file_split[3])[0])\n",
    "            filenames.append(file)\n",
    "            X.append(data)\n",
    "            Y.append(y)\n",
    "            point.append([float(file_split[1]), float(file_split[2])])\n",
    "    z.close()\n",
    "    filenames = np.array(filenames)\n",
    "    X = np.array(X).astype('float32')\n",
    "    Y = np.array(Y).astype('float32')\n",
    "    print(X.shape, Y.shape)\n",
    "    return filenames, X, Y, point\n",
    "\n",
    "\n",
    "def region_visualizer(df):\n",
    "    points = df[3]\n",
    "    lon, lat = [], []\n",
    "    for point in points:\n",
    "        lon.append(point[0])\n",
    "        lat.append(point[1])\n",
    "    lon = np.array(lon).astype(float).reshape(-1,1)\n",
    "    lat = np.array(lat).astype(float).reshape(-1,1)\n",
    "    region = df[4]\n",
    "    region = np.array(region).astype(int).reshape(-1,1)\n",
    "    df = pd.DataFrame(np.concatenate([lon, lat, region], axis=1))\n",
    "    df.columns = ['longitude', 'latitude', 'region_class']\n",
    "    pivotted = df.pivot('longitude', 'latitude', 'region_class')\n",
    "    for i in range(pivotted.shape[0]):\n",
    "        pivotted.iloc[i] = pd.to_numeric(pivotted.iloc[i])\n",
    "    pivotted.columns = pd.to_numeric(pivotted.columns)\n",
    "    pivotted.index = pd.to_numeric(pivotted.index)\n",
    "    pivotted = pivotted.fillna(-1)\n",
    "    pivotted = pivotted.astype(float).T\n",
    "    cmap = sns.color_palette(\"deep\", cvs + 1)\n",
    "    cmap[0] = (0,0,0)\n",
    "    plt = sns.heatmap(pivotted, cmap = cmap)\n",
    "    plt.invert_yaxis()\n",
    "    colorbar = plt.collections[0].colorbar\n",
    "    r = colorbar.vmax - colorbar.vmin\n",
    "    colorbar.set_ticks([colorbar.vmin + 0.5 * r / (cvs + 1) + r * i / (cvs + 1) for i in range(cvs + 1)])\n",
    "    colorbar.set_ticklabels(['background']+list(range(cvs)))\n",
    "    plt.figure.savefig(\"./result_img/region_map.jpg\")\n",
    "    del(plt)\n",
    "\n",
    "\n",
    "def data_splitter_cv(X, Y, point, cv, region):\n",
    "    #test_index = np.where(region==cv)\n",
    "    test_index = np.arange(cv, X.shape[0], cvs)\n",
    "    #train_index = np.where(region!=cv)\n",
    "    train_index = np.setdiff1d(np.arange(0, X.shape[0], 1), test_index)\n",
    "    X_test = X[test_index]\n",
    "    y_test = Y[test_index]\n",
    "    X_train = X[train_index]\n",
    "    y_train = Y[train_index]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Loss Definition ----------------------------------\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis = -1))\n",
    "\n",
    "\n",
    "def create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out):\n",
    "    inputs = Input(image_shape)\n",
    "    with tf.device('/gpu:0'):\n",
    "        x = Dropout(dropout_rate_in)(inputs)\n",
    "        x = Convolution2D(filters = 2**num_filters[0], kernel_size = (size_filters[0],size_filters[0]), padding = 'same', activation = 'relu')(x)\n",
    "        for i in range(1, num_layer):\n",
    "            x = Convolution2D(filters = 2**num_filters[i],\n",
    "                              kernel_size = (size_filters[i], size_filters[i]),\n",
    "                              padding = padding,\n",
    "                              activation = 'relu')(x)\n",
    "            x = MaxPooling2D()(x)\n",
    "        x = Convolution2D(filters = 7,\n",
    "                          kernel_size = (3, 3),\n",
    "                          padding = 'same',\n",
    "                          activation = 'relu')(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(units = 2**dense_num, activation = 'relu')(x)\n",
    "        x = Dropout(dropout_rate_out)(x)\n",
    "        x = Dense(units = 2, activation = 'softmax')(x)\n",
    "        model = Model(inputs = inputs, outputs = x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def opt_cnn(trial):\n",
    "    # Opt params -----------------------\n",
    "    # Categorical parameter\n",
    "    num_layer = trial.suggest_int('num_layer', 1, 2)\n",
    "    dense_num = trial.suggest_int('dense_num', 2, 5)\n",
    "    num_filters = [int(trial.suggest_discrete_uniform('num_filter_' + str(i), 2, 5, 1)) for i in range(num_layer)]\n",
    "    size_filters = [int(trial.suggest_discrete_uniform('size_filter_' + str(i), 3, 5, 2)) for i in range(num_layer)]\n",
    "    batch_size = trial.suggest_int('batch_size', 1, 5)\n",
    "    # Model Compiler -----------------------\n",
    "    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    decay = trial.suggest_loguniform('decay', 1e-6, 1e-3)\n",
    "    # Discrete-uniform parameter\n",
    "    dropout_rate_in = trial.suggest_discrete_uniform('dropout_rate_in', 0.0, 0.5, 0.1)\n",
    "    dropout_rate_out = trial.suggest_discrete_uniform('dropout_rate_out', 0.0, 0.5, 0.1)\n",
    "    momentum = trial.suggest_discrete_uniform('momentum', 0.0, 1.0, 0.1)\n",
    "    # categorical parameter\n",
    "#    optimizer = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\", \"rmsprop\", \"adam\"])\n",
    "    padding = trial.suggest_categorical('padding', ['same', 'valid'])\n",
    "    # compile model-------------------\n",
    "    model = create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out)\n",
    "    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True)\n",
    "#    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True, clipvalue = 1.0)\n",
    "    # For CPU run ------------------\n",
    "    model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Train Model ----------------------------------\n",
    "    es_cb = EarlyStopping(monitor = 'val_loss', patience = early_stopping, verbose = 1)\n",
    "    pr_cb = integration.TFKerasPruningCallback(trial, 'val_loss')\n",
    "    cbs = [es_cb, pr_cb]\n",
    "    loss_list = []\n",
    "    for inner_cv in tqdm(range(0, cvs)):\n",
    "        X_val_train, X_val, y_val_train_sparse, y_val_sparse = data_splitter_cv(X_train, y_train_sparse, point, inner_cv, region)\n",
    "        hist = model.fit(\n",
    "            train_datagen.flow(X_val_train, y_val_train_sparse, batch_size = (2**batch_size) * gpus),\n",
    "            epochs = train_epochs,\n",
    "            validation_data = (X_val, y_val_sparse),\n",
    "            callbacks = cbs,\n",
    "            shuffle = True,\n",
    "            verbose = 1,\n",
    "            use_multiprocessing = False)\n",
    "        loss_list += [model.evaluate(X_val, y_val_sparse)[0]]\n",
    "    del model\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    eval = np.mean(loss_list)\n",
    "    return eval\n",
    "\n",
    "\n",
    "def mean_params_calc(param_names):\n",
    "    dict = {}\n",
    "    categoricals = ['padding']\n",
    "    for param_name in param_names:\n",
    "        data_num = 0\n",
    "        if param_name not in categoricals:\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    try:\n",
    "                        dict[param_name] += data[param_name]\n",
    "                    except:\n",
    "                        dict[param_name] = data[param_name]\n",
    "                    data_num = data_num + 1\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = dict[param_name]/data_num\n",
    "        else:\n",
    "            categorical_list = []\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    categorical_list = categorical_list + [data[param_name]]\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = stats.mode(categorical_list)[0][0]\n",
    "    return dict\n",
    "\n",
    "\n",
    "def cv_result_imgs_generator(model, history):\n",
    "    # Visualize Loss Results ----------------------------\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(history.history[\"loss\"], label=\"loss\", marker=\"o\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\", marker=\"o\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.title(\"\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(color='gray', alpha=0.2)\n",
    "    plt.savefig('./result_img/' + str(cv) + '_loss.jpg')\n",
    "    plt.close()\n",
    "    # Train data -----------------------\n",
    "    plt.figure()\n",
    "#    y_train_preds = model.predict(X_train)\n",
    "    y_train_preds = [np.dot(np.arange(num_classes), model.predict(X_train)[i])*10 for i in range(len(y_train))]\n",
    "    plt.scatter(y_train, y_train_preds, s=3, alpha=0.5)\n",
    "    plt.xlim(min([np.min(y_train_preds), np.min(y_train)]), max([np.max(y_train_preds), np.max(y_train)]))\n",
    "    plt.xlabel(\"obs\")\n",
    "    plt.ylabel(\"pred\")\n",
    "    x = np.linspace(min([np.min(y_train_preds), np.min(y_train)]), max([np.max(y_train_preds), np.max(y_train)]), 100)\n",
    "    y = x\n",
    "    plt.plot(x, y, 'r-')\n",
    "    plt.savefig('./result_img/' + str(cv) + '_scatter_train.jpg')\n",
    "    plt.close()\n",
    "    # Evaluate test data -----------------------\n",
    "    plt.figure()\n",
    "    #y_preds = model.predict(X_val)\n",
    "    y_preds = [np.dot(np.arange(num_classes), model.predict(X_val)[i])*10 for i in range(len(y_val))]\n",
    "    plt.scatter(y_val, y_preds, s=3, alpha=0.5)\n",
    "    plt.xlim(min([np.min(y_val),np.min(y_preds)]), max([np.max(y_val), np.max(y_preds)]))\n",
    "    plt.ylim(min([np.min(y_val),np.min(y_preds)]), max([np.max(y_val), np.max(y_preds)]))\n",
    "    plt.xlabel(\"obs\")\n",
    "    plt.ylabel(\"pred\")\n",
    "    x = np.linspace(min([np.min(y_val),np.min(y_preds)]), max([np.max(y_val), np.max(y_preds)]), 100)\n",
    "    y = x\n",
    "    plt.plot(x, y, \"r-\")\n",
    "    plt.savefig('./result_img/' + str(cv) + '_scatter_test.jpg')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generalization_result_imgs_generator(name, y_val_pred, y_val_all):\n",
    "    # Evaluate test data -----------------------\n",
    "    plt.figure()\n",
    "    plt.scatter(y_val_all, y_val_pred, s=3, alpha=0.5)\n",
    "    plt.xlim(min([np.min(y_val_all), np.min(y_val_pred)]), max([np.max(y_val_all),np.max(y_val_pred)]))\n",
    "    plt.xlabel(\"obs\")\n",
    "    plt.ylabel(\"pred\")\n",
    "    x = np.linspace(min([np.min(y_val_all), np.min(y_val_pred)]), max([np.max(y_val_all),np.max(y_val_pred)]),100)\n",
    "    y = x\n",
    "    plt.plot(x, y, \"r-\")\n",
    "    plt.savefig('./result_img/' + name + '_scatter_test.jpg')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def categorical_converter(Y):\n",
    "    Y = Y/10\n",
    "    Y = Y.astype(np.int8)\n",
    "    return Y\n",
    "\n",
    "def binary_converter(Y):\n",
    "    Y = Y/4\n",
    "    Y = Y.astype(np.int8)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader ------------------------------\n",
    "train_zip_name = 'train_crowns.zip'\n",
    "test_zip_name = 'test_crowns.zip'\n",
    "image_size = 30\n",
    "cvs = 10\n",
    "train_epochs = 64\n",
    "ntrials = 32\n",
    "early_stopping = 16\n",
    "best_epochs = 256\n",
    "# Train_DataFrame_Generator ----------------------\n",
    "filenames, X, Y, point = crown_DataLoader(train_zip_name)\n",
    "region = KMeans(n_clusters = cvs, random_state=0).fit(point).labels_\n",
    "filenames_test, X_test, y_test, point_test = crown_DataLoader(test_zip_name)\n",
    "X_mean, Y_mean = X.mean(), Y.mean()\n",
    "X_std, Y_std = X.std(), Y.std()\n",
    "#X = (X - X_mean)/X_std\n",
    "#Y = (Y - Y_mean)/Y_std\n",
    "#X_test = (X_test - X_mean)/X_std\n",
    "#y_test = (y_test - Y_mean)/Y_std\n",
    "df = [filenames, X, Y, point, region]\n",
    "filenames, X, Y, point, region = df[0], df[1], df[2], df[3], df[4]\n",
    "image_shape = (X.shape[1], X.shape[2], X.shape[3])\n",
    "standardization_params = [X_mean, Y_mean, X_std, Y_std]\n",
    "# Training Settings --------------------------------------\n",
    "# Data Augmentation --------------------------------\n",
    "dir_generator()\n",
    "region_visualizer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model ----------------------------------\n",
    "# CV start ------------------------------------------------------------\n",
    "train_results, best_params = [], []\n",
    "for cv in tqdm(range(cvs)):\n",
    "    print('cv_' + str(cv) + '_processing....')\n",
    "    # Data Loader-------------------------------------\n",
    "    X_train, X_val, y_train, y_val = data_splitter_cv(X, Y, point, cv, region)\n",
    "    y_train_sparse = categorical_converter(y_train)\n",
    "    y_train_sparse = binary_converter(y_train_sparse)\n",
    "    y_val_sparse = categorical_converter(y_val)\n",
    "    y_val_sparse = binary_converter(y_val_sparse)\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range = 360,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True)\n",
    "    test_datagen = ImageDataGenerator()\n",
    "    # Bayesian optimization -------------------------------------\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(opt_cnn, n_trials = ntrials)\n",
    "    # Best_model_training ---------------------------------------\n",
    "    best_params.append(study.best_params)\n",
    "    num_filters = [int(study.best_params['num_filter_' + str(i)]) for i in range(int(study.best_params['num_layer']))]\n",
    "    size_filters = [int(study.best_params['size_filter_' + str(i)]) for i in range(int(study.best_params['num_layer']))]\n",
    "    model = create_model(image_shape, int(study.best_params['num_layer']), study.best_params['padding'], int(study.best_params['dense_num']), num_filters, size_filters, study.best_params['dropout_rate_in'], study.best_params['dropout_rate_out'])\n",
    "    sgd = optimizers.SGD(lr = study.best_params['learning_rate'], decay = study.best_params['decay'], momentum = study.best_params['momentum'], nesterov = True, clipvalue = 1.0)\n",
    "    model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        train_datagen.flow(X_train, y_train_sparse, batch_size = 2**int(study.best_params['batch_size']) * gpus),\n",
    "        epochs = train_epochs,\n",
    "        validation_data = (X_val, y_val_sparse),\n",
    "        shuffle = True,\n",
    "        verbose = 1,\n",
    "        use_multiprocessing = False)\n",
    "    try:\n",
    "        y_val_pred = np.concatenate((y_val_pred, [np.dot(np.arange(num_classes), model.predict(X_val)[i])*10 for i in range(len(y_val))]))\n",
    "    except:\n",
    "        y_val_pred = [np.dot(np.arange(num_classes), model.predict(X_val)[i])*10 for i in range(len(y_val))]\n",
    "    try:\n",
    "        y_val_all = np.concatenate((y_val_all, y_val))\n",
    "    except:\n",
    "        y_val_all = y_val\n",
    "        y_val_pred_classes = model.predict(X_val).argmax(axis=1)\n",
    "    y_val_pred_classes = model.predict(X_val).argmax(axis=1)\n",
    "    try:\n",
    "        y_val_smx = y_val_smx + confusion_matrix(y_val_sparse, y_val_pred_classes)\n",
    "    except:\n",
    "        y_val_smx = confusion_matrix(y_val_sparse, y_val_pred_classes)\n",
    "    cv_result_imgs_generator(model, history)\n",
    "    #compare_TV(history, cv)\n",
    "    del model\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CV_Result -------------------------------------------------\n",
    "generalization_result_imgs_generator('val', y_val_pred, y_val_all)\n",
    "np.savetxt('y_val_smx.txt', y_val_smx)\n",
    "pkl_saver(train_results, 'train_results.binaryfile')\n",
    "pkl_saver(standardization_params, 'standardization_params.binaryfile')\n",
    "param_names = best_params[list(map(len, best_params)).index(max(list(map(len, best_params))))].keys()\n",
    "best_params_dict = mean_params_calc(param_names)\n",
    "pkl_saver(best_params, 'best_params_list.binaryfile')\n",
    "pkl_saver(best_params_dict, 'best_params.binaryfile')\n",
    "best_params_dict = pkl_loader('best_params.binaryfile')\n",
    "\n",
    "# Best Model Training -----------------------------------------------\n",
    "# Int parameter\n",
    "num_layer = int(best_params_dict['num_layer'])\n",
    "num_filters = [int(best_params_dict['num_filter_' + str(i)]) for i in range(num_layer)]\n",
    "size_filters = [int(best_params_dict['size_filter_' + str(i)]) for i in range(num_layer)]\n",
    "dense_num = int(best_params_dict['dense_num'])\n",
    "batch_size = int(best_params_dict['batch_size'])\n",
    "# Uniform parameter\n",
    "# Loguniform parameter\n",
    "lr = best_params_dict['learning_rate']\n",
    "decay = best_params_dict['decay']\n",
    "# Discrete-uniform parameter\n",
    "dropout_rate_in = best_params_dict['dropout_rate_in']\n",
    "dropout_rate_out = best_params_dict['dropout_rate_out']\n",
    "momentum = best_params_dict['momentum']\n",
    "# Categorical parameter\n",
    "padding = best_params_dict['padding']\n",
    "\n",
    "\n",
    "# Model Checkpoint ------------------\n",
    "cp_cb = ModelCheckpoint(\n",
    "    './weights/best_weights.hdf5',\n",
    "    monitor = 'val_loss',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True,\n",
    "    mode = 'auto')\n",
    "# Logging ----------------------------------------\n",
    "log_dir = os.path.join('./logs/')\n",
    "tb_cb = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "es_cb = EarlyStopping(monitor = 'val_loss', patience = int(best_epochs/10), verbose = 1)\n",
    "\n",
    "cbs = [cp_cb, tb_cb, es_cb]\n",
    "\n",
    "\n",
    "# Train Best_Model ----------------------------------\n",
    "# For CPU run ------------------\n",
    "best_model = create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out)\n",
    "sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True, clipvalue = 1.0)\n",
    "\n",
    "Y_sparse = categorical_converter(Y)\n",
    "Y_sparse = binary_converter(Y_sparse)\n",
    "\n",
    "best_model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy')\n",
    "hist = best_model.fit(\n",
    "    train_datagen.flow(X, Y_sparse, batch_size = (2**batch_size) * gpus),\n",
    "    epochs = best_epochs,\n",
    "    callbacks = cbs,\n",
    "    shuffle = True,\n",
    "    verbose = 1,\n",
    "    initial_epoch = 0,\n",
    "    use_multiprocessing = False)\n",
    "\n",
    "# Save Model -----------------------------------\n",
    "#best_model.save('./model/best_model.hdf5')\n",
    "\n",
    "# Unknown Predictor ------------------------------------------------\n",
    "# Load Pre-trained Model -------------------------------------\n",
    "#best_model = load_model('./model/best_model.hdf5', custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
    "\n",
    "# Data Loader ------------------------------\n",
    "#y_test_pred = [np.dot(np.arange(num_classes), best_model.predict(X_test)[i])*10 for i in range(len(y_test))]\n",
    "y_test_pred = [np.dot(np.array([40,60]), best_model.predict(X_test)[i])*10 for i in range(len(y_test))]\n",
    "#y_test_pred = y_test_pred*Y_std + Y_mean\n",
    "#y_test = y_test*Y_std + Y_mean\n",
    "#generalization_result_imgs_generator('test', y_test_pred, y_test)\n",
    "np.savetxt('y_test_pred.txt', y_test_pred)\n",
    "with open(\"best_model_summary.txt\", \"w\") as fp:\n",
    "    best_model.summary(print_fn=lambda x: fp.write(x + \"\\r\\n\"))\n",
    "\n",
    "#sum_y_pred = np.sum(y_test_pred*Y_std+Y_mean)\n",
    "#sum_y_test = np.sum(y_test*Y_std+Y_mean)\n",
    "\n",
    "print('finished...')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
